<!doctype html>
<html>

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LIT Lab‚Äôs Learning Portal</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-1NG1BPBBQ1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-1NG1BPBBQ1');
  </script>

  <link rel="stylesheet" href="../case-study.css">

  <!-- fonts -->
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Rubik:wght@400;500;600;700;800;900&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;500;600;700;800;900&display=swap" rel="stylesheet">

  <style>
     :root {
      --title-font: "Rubik", sans-serif;
      --body-font: 'Rubik', sans-serif;
    }
    /** new styles **/
    
    .ufo--case-study-container {
      --h1Font: "Rubik";
      --h1Color: #353738;
      --h1Weight: bold;
      --h1Style: normal;
      --h1Size: 24px;
      --h1Height: 25px;
      --h2Font: "Rubik";
      --h2Color: #353738;
      --h2Weight: normal;
      --h2Style: italic;
      --h2Size: 28px;
      --h2Height: 30px;
      --h3Font: "Rubik";
      --h3Color: #353738;
      --h3Weight: bold;
      --h3Style: normal;
      --h3Size: 21px;
      --h3Height: 25px;
      --h4Font: "Rubik";
      --h4Color: #353738;
      --h4Weight: normal;
      --h4Style: normal;
      --h4Size: 17px;
      --h4Height: 20px;
      --p1Font: "Open Sans";
      --p1Color: #353738;
      --p1Weight: normal;
      --p1Style: normal;
      --p1Size: 16px;
      --p1Height: 32px;
      --p2Font: "Rubik";
      --p2Color: #7b7e80;
      --p2Weight: normal;
      --p2Style: normal;
      --p2Size: 16px;
      --p2Height: 16px;
      --projectTitleFont: "Rubik";
      --projectTitleColor: #252829;
      --projectTitleWeight: bold;
      --projectTitleStyle: normal;
      --projectTitleSize: 35px;
      --projectTitleHeight: 45px;
      --projectSubTitleFont: "Open Sans";
      --projectSubTitleColor: #252829;
      --projectSubTitleWeight: normal;
      --projectSubTitleStyle: normal;
      --projectSubTitleSize: 16px;
      --projectSubTitleHeight: 28px;
    }
    
    @media (min-width: 576px) {
      .ufo--case-study-container {
        --h3Size: 25px;
        --h3Height: 28px;
        /* --p1Size: 18px;
        --p1Height: 21px; */
        --projectTitleSize: 48px;
        --projectTitleHeight: 51px;
        --projectSubTitleSize: 22px;
        --projectSubTitleHeight: 37px;
      }
    }
    
    @media (min-width: 768px) {
      .ufo--case-study-container {
        --h1Size: 60px;
        --h1Height: 63px;
        --h2Size: 40px;
        --h2Height: 43px;
        --h3Size: 30px;
        --h3Height: 36px;
        --h4Size: 24px;
        --h4Height: 29px;
        /* --p1Size: 18px;
        --p1Height: 30px; */
        --p2Size: 16px;
        --p2Height: 19px;
        --projectTitleSize: 88px;
        --projectTitleHeight: 91px;
        --projectSubTitleSize: 22px;
        --projectSubTitleHeight: 37px;
      }
    }
    
    .p_section--header-main .ufo--hero-content-grid.has-image .text-container {
      text-align: center;
    }
    
    .p_section--header-main .text-container {
      text-align: center;
    }
    
    @media (min-width: 768px) {}
    
    @media (min-width: 992px) {
      .p_section--header-main .ufo--hero-content-grid.has-image .text-container {
        text-align: left;
      }
    }
    
    td {
      display: table-cell;
      align-items: center;
      /* border: 1px dotted red;
      background-color: red; */
    }
    
    .hero-main-content {
      max-width: 1240px;
      padding: 0px 20px 0px 20px;
      margin-left: auto;
      margin-right: auto;
    }
    
    .hero-text {
      width: 60%;
      padding-right: 20px;
    }
    
    .hero-image {
      text-align: center;
      vertical-align: middle;
    }
    
    .hero-image img {
      height: 450px;
      width: auto;
    }
    
    @media only screen and (max-width: 768px) {
      td {
        display: block;
      }
      .hero-text {
        width: 100%;
        text-align: center;
        margin-top: 10px;
      }
      .hero-image img {
        width: 50%;
        height: auto;
        margin-top: 20px;
        margin-bottom: 30px;
      }
      .hero-text h1 {
        margin-bottom: 10px;
      }
    }
  </style>
</head>

<body>
  <main>
    <div class="ufo--case-study-container project">

      <section id="section-0" class="ufo--case-study-section p_section p_section--header-main  " style="background-color: #E5C7CE; margin-bottom: 40px;">
        <div class="ufo--p-container-fluid ">
          <div class="project-owner">
            <div class="owner" onclick="window.location.href='../'">
              <div class="owner-info ">
                <div class="owner-name">Home</div>
              </div>
            </div>
          </div>

          <table class="hero-main-content" width="100%" height="100%" style="padding-top: 5%;" cellpadding="0" cellspacing="0">
            <tbody>
              <tr>
                <td class="hero-text">
                  <h1>LIT Lab‚Äôs Learning Portal</h1>
                  <h2>An educational website that teaches multimodal analytics to students and captures their multimodal data in the background.</h2>
                </td>
                <td class="hero-image">
                  <img src="../img/book-bug-cropped.png">
                </td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>‚úèÔ∏è Overview</h3>
            <p>
              I am working with a professor at Harvard University to develop a <a target="_blank" href="https://lp.gse.harvard.edu/signin">learning portal website</a> for graduate students enrolled in a Multimodal Analytics course. This project was done
              at the <a target="_blank" href="https://lit.gse.harvard.edu/">Learning, Innovation, and Technology (LIT) lab</a> based at the Harvard Graduate School of Education. The learning portal captures multimodal data from learners, namely body posture,
              gaze orientation, and emotion, as they watch instructional videos. We plan to study this data in the future to advance our understanding of students‚Äô state as they learn from video material.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üë©üèª‚Äçüíª My Role</h3>
            <p>
              I am the sole developer of this project and built the application from the ground up within a 1-month deadline.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üîé Background</h3>
            <p>
              Advanced computational analysis of multimodal data, also known as Multimodal Learning Analytics (MMLA), is a growing field of research that has helped researchers better understand and model complex learning processes usually overlooked by conventional
              approaches. Insights from multimodal data can be utilized by teachers to adjust their teaching practices and maximize student learning. The multimodal data itself can also be provided to learners as personal data to study.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üö© The Problem</h3>
            <p>
              Multimodal data in online education generally involves students‚Äô interaction data with learning platforms, such as Learning Management Systems and Massive Online Open Courses. Traditionally, these data sources only capture the learner‚Äôs interactions with
              the learning system as log files and hardly capture any other learning moments. Data streams such as video and audio can provide richer information on students‚Äô learning using machine learning and computer vision algorithms, and this data
              can be tracked through built-in webcams and microphones in computers. However, data privacy and ethical requirements make them challenging to use, and students may not feel comfortable having their raw video and audio data stored and processed
              externally.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üß© Our Solution</h3>
            <p>
              To address the challenges stated above, we are designing a learning portal website. The website provides instructional videos and simultaneously captures multimodal data from students, namely body posture, gaze location, and emotion, while they use the
              platform. The EZ-MMLA Toolkit uses machine learning models written in JavaScript to make predictions from video and audio input that run entirely within the web browser. A significant advantage of such models is that they can process input
              streams within the user‚Äôs browser, so personal data does not need to leave the user‚Äôs devices. This means that student webcam and audio recordings remain within their own devices, and students do not have to worry about the security concerns
              of sending sensitive recordings over a network. Only the processed data generated by the models is stored externally, to be used by researchers and educators to assess student engagement and improve teaching practices.

              <br><br>This data is complemented by manual user inputs: the platform provides learners with the option to indicate their affective states while watching the video - for example, if they feel confused, frustrated, lost, interested, inspired,
              or experience an ‚Äòaha‚Äô moment. Additionally, we use weekly quizzes to measure learning and surveys to measure students‚Äô emotional state, belonging, and sense of agency. This provides us with ground truth for the multimodal measures. Because
              this data is collected every time students watch videos, we can use it to frequently iterate on the course design. The base code for the learning portal will be published under an open-source license at the end of this project, so that any
              educator can use it to collecting multimodal data while delivering instructional videos.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>‚úçÔ∏è User Feedback</h3>
            <p>
              The learning portal is currently being used to teach a Multimodal Learning Analytics course to graduate students at the Harvard Graduate School of Education. Students used the learning portal as part of an adapted version of an in-person course that had
              transitioned online because of the ongoing COVID-19 pandemic. Students studied their own multimodal data generated from the portal and learned to apply supervised and unsupervised machine learning algorithms. We gathered feedback throughout
              the semester using short open-ended surveys.

              <br><br>To evaluate our prototype platform, we gathered qualitative feedback from the students that trialed the platform. When asked about the multimodal data collection website and the learning portal, students gave particularly positive
              feedback on collecting data first-hand and expressed a strong interest in analyzing their data. Many said that they appreciate ‚Äúhaving the opportunity to work with data collection directly [since] it helps [them] to understand limitations
              of data and the entire concept much more than [they] would have‚Äù, and that ‚Äúit allowed [them] to feel more invested in the final product‚Äù. Additionally, we find that through the data collection process, students became more engaged in the
              underlying models and began to explore broader implications of their observations. Specifically, students reported that they ‚Äúenjoyed seeing how emotion can be detected and analyzed‚Äù and grew more interested in ‚Äúwhat multimodal sensors can
              tell us about our teaching and learning‚Äù. With the new tools available and a deeper understanding of data collected, students found both sites to ‚Äúbe a way [they] can practically gather data that is useful for proving theories.‚Äù
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üí° Summary and Future Work</h3>
            <p>
              We designed a learning portal capable of capturing rich multimodal data from video and audio input streams using advanced machine learning algorithms. The data captured through the application provides richer and more comprehensive information on student
              learning than conventionally used clickstream logs. Currently, we are working on analyzing the information gathered through the platform. </p>
          </div>
        </div>
      </section>
    </div>

    <div class="scroll-to-top"></div>

  </main>
</body>

</html>