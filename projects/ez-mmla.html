<!doctype html>
<html>

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>The EZ-MMLA Toolkit</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-1NG1BPBBQ1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-1NG1BPBBQ1');
  </script>

  <link rel="stylesheet" href="../case-study.css">
  <link rel="stylesheet" href="../static/css/sticky_notes.css">

  <!-- fonts -->
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Rubik:wght@400;500;600;700;800;900&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;500;600;700;800;900&display=swap" rel="stylesheet">

  <!-- jquery -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

  <style>
     :root {
      --title-font: "Rubik", sans-serif;
      --body-font: 'Rubik', sans-serif;
    }
    /** new styles **/
    
    .ufo--case-study-container {
      --h1Font: "Rubik";
      --h1Color: #353738;
      --h1Weight: bold;
      --h1Style: normal;
      --h1Size: 24px;
      --h1Height: 25px;
      --h2Font: "Rubik";
      --h2Color: #353738;
      --h2Weight: normal;
      --h2Style: italic;
      --h2Size: 28px;
      --h2Height: 30px;
      --h3Font: "Rubik";
      --h3Color: #353738;
      --h3Weight: bold;
      --h3Style: normal;
      --h3Size: 21px;
      --h3Height: 25px;
      --h4Font: "Rubik";
      --h4Color: #353738;
      --h4Weight: normal;
      --h4Style: normal;
      --h4Size: 17px;
      --h4Height: 20px;
      --p1Font: "Open Sans";
      --p1Color: #353738;
      --p1Weight: normal;
      --p1Style: normal;
      --p1Size: 16px;
      --p1Height: 32px;
      --p2Font: "Rubik";
      --p2Color: #7b7e80;
      --p2Weight: normal;
      --p2Style: normal;
      --p2Size: 16px;
      --p2Height: 25px;
      --projectTitleFont: "Rubik";
      --projectTitleColor: #252829;
      --projectTitleWeight: bold;
      --projectTitleStyle: normal;
      --projectTitleSize: 35px;
      --projectTitleHeight: 45px;
      --projectSubTitleFont: "Open Sans";
      --projectSubTitleColor: #252829;
      --projectSubTitleWeight: normal;
      --projectSubTitleStyle: normal;
      --projectSubTitleSize: 16px;
      --projectSubTitleHeight: 28px;
    }
    
    @media (min-width: 576px) {
      .ufo--case-study-container {
        --h3Size: 25px;
        --h3Height: 28px;
        /* --p1Size: 18px;
        --p1Height: 21px; */
        --projectTitleSize: 48px;
        --projectTitleHeight: 51px;
        --projectSubTitleSize: 22px;
        --projectSubTitleHeight: 37px;
      }
    }
    
    @media (min-width: 768px) {
      .ufo--case-study-container {
        --h1Size: 60px;
        --h1Height: 63px;
        --h2Size: 40px;
        --h2Height: 43px;
        --h3Size: 30px;
        --h3Height: 36px;
        --h4Size: 24px;
        --h4Height: 29px;
        /* --p1Size: 18px;
        --p1Height: 30px; */
        --p2Size: 16px;
        --projectTitleSize: 88px;
        --projectTitleHeight: 91px;
        --projectSubTitleSize: 22px;
        --projectSubTitleHeight: 37px;
      }
    }
    
    .p_section--header-main .ufo--hero-content-grid.has-image .text-container {
      text-align: center;
    }
    
    .p_section--header-main .text-container {
      text-align: center;
    }
    
    @media (min-width: 768px) {}
    
    @media (min-width: 992px) {
      .p_section--header-main .ufo--hero-content-grid.has-image .text-container {
        text-align: left;
      }
    }
    
    td {
      display: table-cell;
      align-items: center;
      /* border: 1px dotted red;
      background-color: red; */
    }
    
    .hero-main-content {
      max-width: 1240px;
      padding: 0px 20px 0px 20px;
      margin-left: auto;
      margin-right: auto;
    }
    
    .hero-text {
      width: 60%;
      padding-right: 20px;
    }
    
    .hero-image {
      text-align: center;
      vertical-align: bottom;
    }
    
    .hero-image img {
      height: 550px;
      width: auto;
    }
    
    @media only screen and (max-width: 768px) {
      td {
        display: block;
      }
      .hero-text {
        width: 100%;
        text-align: center;
        margin-top: 10px;
      }
      .hero-image img {
        width: 50%;
        height: auto;
        margin-top: 20px;
      }
      .hero-text h1 {
        margin-bottom: 10px;
      }
    }
  </style>
</head>

<body>
  <main>
    <div class="ufo--case-study-container project">

      <section id="section-0" class="ufo--case-study-section p_section p_section--header-main  " style="background-color: #aecdbf; margin-bottom: 40px;">
        <div class="ufo--p-container-fluid ">
          <div class="project-owner">
            <div class="owner" onclick="window.location.href='../'">
              <div class="owner-info ">
                <div class="owner-name">Home</div>
              </div>
            </div>
          </div>

          <table class="hero-main-content" width="100%" height="100%" style="padding-top: 5%;" cellpadding="0" cellspacing="0">
            <tbody>
              <tr>
                <td class="hero-text">
                  <h1>The EZ-MMLA Toolkit</h1>
                  <h2>A data collection website that provides educators and researchers with easy access to multimodal data streams.</h2>
                </td>
                <td class="hero-image">
                  <img src="../img/man-cropped.png">
                </td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>‚úèÔ∏è Overview</h3>
            <p>
              I am working with a group of researchers at the <a target="_blank" href="https://lit.gse.harvard.edu/">Learning, Innovation, and Technology lab</a> based at the Harvard Graduate School of Education to develop a <a target="_blank" href="https://mmla.gse.harvard.edu/">multimodal data collection
              website</a>. The website uses state-of-the-art machine learning algorithms to collect data on students‚Äô body posture, hand gestures, attention, emotions, and physiological states. The application is currently being used by 60 Harvard graduate
              students enrolled in a Multimodal Analytics course.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üë©üèª‚Äçüíª My Role</h3>
            <p>
              I am the team lead for this project. I researched the most effective models to use in the application, developed the frontend for 11/15 tools currently available on the website, architected and developed the backend, and deployed the application. I also
              provided technical guidance and mentorship to team members.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üîé Background</h3>
            <p>
              Data has proven valuable in understanding student learning processes and uncovering factors that most affect learning outcomes. Recently, educational researchers have become more interested in new tools for capturing detailed learning trajectories, which
              has led to the creation of a new methodology: Multimodal Learning Analytics (MMLA), involving the computational analysis of multimodal data. Generally, MMLA research focuses on using high-frequency sensors and wearable tracking devices,
              such as eye-trackers (e.g., Tobii eye-trackers) and motion sensors (e.g., Microsoft Kinect), to capture rich physiological and behavioral student data.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üö© The Problem</h3>
            <p>
              While MMLA is a promising field, it is not a widely used methodology in practice because of the specialized equipment and technical knowledge required to collect, process, and analyze multimodal datasets. Multimodal data collection, in particular, involves
              the use of a combination of physical sensors, proprietary tools, and external contractors, which pose the following challenges:
            </p>
          </div>
        </div>
      </section>

      <section id="section-2" class="ufo--case-study-section p_section" style="padding-top: 20px;  padding-bottom: 20px;">
        <div class="ufo--p-container-fluid">
            <div class="text-container">
                <div class="ufo--basic-text-area">
                  <ul class="sticky_notes">
                    <li>
                      <a>
                        <p><b>üí∞ Affordability</b></p>
                        <p>Current sensor hardware is generally not affordable by researchers and teachers. This restricts multimodal data research to well-funded teams or more commercially lucrative research areas.</p>
                      </a>
                    </li>
                    <li>
                      <a>
                        <p><b>üîí Accessibility</b></p>
                        <p>The high-effort and logistically challenging processes of data collection and analysis restrict access to users with a strong technical background.</p>
                      </a>
                    </li>
                    <li>
                      <a>
                        <p><b>üë§ Participant Effects</b></p>
                        <p>Physical sensors can be invasive for participants and lead to Hawthorne effects and similar biases entering data.</p>
                      </a>
                    </li>
                    <li>
                      <a>
                        <p><b>‚ùó Limited research potential</b></p>
                        <p>The complexity and cost of the data collection process make it difficult to develop a system of frequent assessment and feedback, which is an essential process in education.</p>
                      </a>
                    </li>
                  </ul>
                </div>
            </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
              However, with advances in computer vision, machine learning, and computer hardware, multimodal data collection has allowed for new, simplifying possibilities. For instance, computer vision algorithms can now accurately predict heart rate by computing
              minute skin tone differentials. More broadly, machine learning algorithms can now be used to develop easy-to-use and low-cost alternatives to conventional multimodal data collection.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üéØ The Goal</h3>
            <p>
              Our goal is to democratize multimodal data collection using advanced machine learning algorithms behind a user-friendly interface. We want to design a more intuitive, low-cost, and ethical solution compared to traditionally used physical sensor technologies.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üß© Our Solution</h3>
            <p>
              We developed the EZ-MMLA toolkit, which is a website for collecting multimodal data via video and audio input captured either in real-time or uploaded for post-processing. It is free to use and open to the public, so it can be used by all educational
              practitioners and researchers to collect multimodal student data. The types of data currently collected through the website are:
              <ol>
                <li>Body posture (skeletal data),</li>
                <li>Hand gestures,</li>
                <li>Attention (eye-tracking),</li>
                <li>Emotions,</li>
                <li>Physiological states (heart rate), and</li>
                <li>Lower-level computer vision algorithms (fiducial and color tracking).</li>
              </ol>

              The EZ-MMLA Toolkit uses machine learning models written in JavaScript to detect data via video and audio streams that run entirely within the web browser. A significant advantage of such models is that the computation required to train and run them is
              offloaded to the user‚Äôs device. This eliminates the need to maintain an expensive remote machine and allows our website to be scalable. It also allows for rapid real-time inferencing on the client-side, generating more comprehensive data
              and creating smoother user experiences. Furthermore, personal data does not leave the user‚Äôs devices; in our case, this means that webcam recordings remain within the browser and users do not have to worry about the security concerns of
              sending sensitive information over a network.

              <br><br>The EZ-MMLA Toolkit website has been designed with a user-friendly interface that does not require any pre-requisite technical knowledge. On the home page, users can select from an index of data-collection tools that correspond to
              a multimodal data type. When a user selects a tool, they are directed to a page where they can execute data collection in real-time through a webcam/microphone feed or upload a recording. Once the tracking is complete, the user is prompted
              to download the data as a CSV file.

              <br><br> <span style="font-weight: bold;">Comparing the EZ-MMLA toolkit with traditional data collection tools</span>
              <br>While computer-vision algorithms provide us with new ways of capturing multimodal data, they are not interchangeable with physical sensors. Generally, webcam-based data collection tools tend to be less accurate than dedicated sensors,
              but they are easier to access, distribute, setup, and use. They have also been steadily improving, as consumer hardware is becoming faster, higher resolution, and more affordable.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>‚úçÔ∏è User Feedback</h3>
            <p>
              In Fall 2020, the EZ-MMLA toolkit was used to teach a Multimodal Learning Analytics course to students at the Harvard Graduate School of Education, in combination with a learning portal (this specific application of our work, called the LIT Learning Portal,
              is described in more detail over here). Students used both the EZ-MMLA toolkit and the learning portal as part of an adapted version of an in-person course that had transitioned online because of the ongoing COVID-19 pandemic.

              <br><br>We gathered feedback throughout the semester using short open-ended surveys. Students gave positive feedback on collecting data first-hand and expressed a strong interest in analyzing data. Many said they appreciated ‚Äúhaving the
              opportunity to work with data collection directly [since] it helped [them] understand the complete process of data mining much more than [they] would have‚Äù. Additionally, students became more engaged in the underlying models and explored
              broader implications of their observations. Specifically, students reported they grew more interested in ‚Äúwhat multimodal sensors can tell us about our teaching and learning.‚Äù
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üí° Conclusion</h3>
            <p>
              In summary, this project finds that the results from the preliminary examination are encouraging. The platform has been broadly successful and indicative of how the proposed web-based approach might be viewed as a viable new medium for MMLA. Crucially,
              our findings suggest that there is the potential to overturn the conventional MMLA pipeline and democratize learning research.
            </p>
          </div>
        </div>
      </section>
    </div>
    <div class="scroll-to-top"></div>
  </main>
</body>

</html>