<!doctype html>
<html>

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>The EZ-MMLA Toolkit</title>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üë©üèª‚Äçüíª</text></svg>">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-1NG1BPBBQ1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-1NG1BPBBQ1');
  </script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script>
    $(function() {
      var includes = $('[data-include]')
      $.each(includes, function() {
        var file = $(this).data('include') + '.html'
        $(this).load(file)
      })
    })
  </script>

  <link rel="stylesheet" href="../style.css">
  <link rel="stylesheet" href="../style_extra.css">
  <link rel="stylesheet" href="../case-study.css">
  <link rel="stylesheet" href="../static/css/sticky_notes.css">

  <!-- fonts -->
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Rubik:wght@400;500;600;700;800;900&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;500;600;700;800;900&display=swap" rel="stylesheet">

  <!-- jquery -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

  <style>
     :root {
      --title-font: "Rubik", sans-serif;
      --body-font: 'Rubik', sans-serif;
    }
    /** new styles **/
    
    .ufo--case-study-container {
      --h1Font: "Rubik";
      --h1Color: #353738;
      --h1Weight: bold;
      --h1Style: normal;
      --h1Size: 24px;
      --h1Height: 25px;
      --h2Font: "Rubik";
      --h2Color: #353738;
      --h2Weight: normal;
      --h2Style: italic;
      --h2Size: 28px;
      --h2Height: 30px;
      --h3Font: "Rubik";
      --h3Color: #353738;
      --h3Weight: bold;
      --h3Style: normal;
      --h3Size: 21px;
      --h3Height: 25px;
      --h4Font: "Rubik";
      --h4Color: #353738;
      --h4Weight: normal;
      --h4Style: normal;
      --h4Size: 17px;
      --h4Height: 20px;
      --p1Font: "Open Sans";
      --p1Color: #353738;
      --p1Weight: normal;
      --p1Style: normal;
      --p1Size: 16px;
      --p1Height: 32px;
      --p2Font: "Rubik";
      --p2Color: #7b7e80;
      --p2Weight: normal;
      --p2Style: normal;
      --p2Size: 16px;
      --p2Height: 25px;
      --projectTitleFont: "Rubik";
      --projectTitleColor: #252829;
      --projectTitleWeight: bold;
      --projectTitleStyle: normal;
      --projectTitleSize: 35px;
      --projectTitleHeight: 45px;
      --projectSubTitleFont: "Open Sans";
      --projectSubTitleColor: #252829;
      --projectSubTitleWeight: normal;
      --projectSubTitleStyle: normal;
      --projectSubTitleSize: 16px;
      --projectSubTitleHeight: 28px;
    }
    
    @media (min-width: 576px) {
      .ufo--case-study-container {
        --h3Size: 25px;
        --h3Height: 28px;
        /* --p1Size: 18px;
        --p1Height: 21px; */
        --projectTitleSize: 48px;
        --projectTitleHeight: 51px;
        --projectSubTitleSize: 22px;
        --projectSubTitleHeight: 37px;
      }
    }
    
    @media (min-width: 768px) {
      .ufo--case-study-container {
        --h1Size: 60px;
        --h1Height: 63px;
        --h2Size: 40px;
        --h2Height: 43px;
        --h3Size: 30px;
        --h3Height: 36px;
        --h4Size: 24px;
        --h4Height: 29px;
        /* --p1Size: 18px;
        --p1Height: 30px; */
        --p2Size: 16px;
        --projectTitleSize: 88px;
        --projectTitleHeight: 91px;
        --projectSubTitleSize: 22px;
        --projectSubTitleHeight: 37px;
      }
    }
    
    .p_section--header-main .ufo--hero-content-grid.has-image .text-container {
      text-align: center;
    }
    
    .p_section--header-main .text-container {
      text-align: center;
    }
    
    @media (min-width: 768px) {}
    
    @media (min-width: 992px) {
      .p_section--header-main .ufo--hero-content-grid.has-image .text-container {
        text-align: left;
      }
    }
    
    td {
      display: table-cell;
      align-items: center;
      /* border: 1px dotted red;
      background-color: red; */
    }
    
    .hero-main-content {
      max-width: 1240px;
      padding: 0px 20px 0px 20px;
      margin-left: auto;
      margin-right: auto;
    }
    
    .hero-text {
      width: 60%;
      padding-right: 20px;
    }
    
    .hero-image {
      text-align: center;
      vertical-align: bottom;
    }
    
    .hero-image img {
      height: 550px;
      width: auto;
    }
    
    @media only screen and (max-width: 768px) {
      .hero-main-content td {
        display: block;
      }
      .hero-text {
        width: 100%;
        text-align: center;
        margin-top: 10px;
      }
      .hero-image img {
        width: 50%;
        height: auto;
        margin-top: 20px;
      }
      .hero-text h1 {
        margin-bottom: 10px;
      }
    }
  </style>
</head>

<body>
  <main>
    <div class="ufo--case-study-container project">

      <section id="section-0" class="ufo--case-study-section p_section p_section--header-main  " style="background-color: #aecdbf; margin-bottom: 40px;">
        <div class="ufo--container">
          <div class="my-ufo--page-navbar-container">
            <div class="ufo--page-navbar">
              <div class="ufo--page-navbar-container" data-include="navbar"></div>
              <div class="ufo--page-navbar-toggle"><span></span><span></span><span></span><span></span></div>
            </div>
          </div>

          <table class="hero-main-content" width="100%" height="100%" style="padding-top: 5%;" cellpadding="0" cellspacing="0">
            <tbody>
              <tr>
                <td class="hero-text">
                  <h1>The EZ-MMLA Toolkit</h1>
                  <h2>A data collection website that provides educators and researchers with easy access to multimodal data streams.</h2>
                </td>
                <td class="hero-image">
                  <img src="../img/man-cropped.png">
                </td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>‚úèÔ∏è Overview</h3>
            <p>
              I am working with a group of researchers at the <a target="_blank" href="https://lit.gse.harvard.edu/">Learning, Innovation, and Technology lab</a> based at the Harvard Graduate School of Education to develop a <a target="_blank" href="https://mmla.gse.harvard.edu/">multimodal data collection
              website</a>. The website uses state-of-the-art machine learning algorithms to collect data on students‚Äô body posture, hand gestures, attention, emotions, and physiological states. The application is currently being used by 100+ Harvard graduate
              students enrolled in a Multimodal Analytics course.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üë©üèª‚Äçüíª My Role</h3>
            <p>
              I am the team lead for this project. I researched the most effective models to use in the application, developed the frontend for the majority of tools currently available on the website, architected and developed the backend, and deployed the application.
              I also provided technical guidance and mentorship to team members.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üîé Background</h3>
            <p>
              Data has proven valuable in understanding student learning processes and uncovering factors that most affect learning outcomes. Recently, educational researchers have become more interested in new tools for capturing detailed learning trajectories, which
              has led to the creation of a new methodology: Multimodal Learning Analytics (MMLA), involving the computational analysis of multimodal data. Generally, MMLA research focuses on using high-frequency sensors and wearable tracking devices,
              such as eye-trackers (e.g., Tobii eye-trackers) and motion sensors (e.g., Microsoft Kinect), to capture rich physiological and behavioral student data.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üö© The Problem</h3>
            <p>
              While MMLA is a promising field, it is not a widely used methodology in practice because of the specialized equipment and technical knowledge required to collect, process, and analyze multimodal datasets. Multimodal data collection, in particular, involves
              the use of a combination of physical sensors, proprietary tools, and external contractors, which pose the following challenges:
            </p>
          </div>
        </div>
      </section>

      <section id="section-2" class="ufo--case-study-section p_section" style="padding-top: 20px;  padding-bottom: 20px;">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <div class="ufo--basic-text-area">
              <ul class="sticky_notes">
                <li>
                  <a>
                    <p><b>üí∞ Affordability</b></p>
                    <p>Current sensor hardware is generally not affordable by researchers and teachers. This restricts multimodal data research to well-funded teams or more commercially lucrative research areas.</p>
                  </a>
                </li>
                <li>
                  <a>
                    <p><b>üîí Accessibility</b></p>
                    <p>The high-effort and logistically challenging processes of data collection and analysis restrict access to users with a strong technical background.</p>
                  </a>
                </li>
                <li>
                  <a>
                    <p><b>üë§ Participant Effects</b></p>
                    <p>Physical sensors can be invasive for participants and lead to Hawthorne effects and similar biases entering data.</p>
                  </a>
                </li>
                <li>
                  <a>
                    <p><b>‚ùó Limited research potential</b></p>
                    <p>The complexity and cost of the data collection process make it difficult to develop a system of frequent assessment and feedback, which is an essential process in education.</p>
                  </a>
                </li>
              </ul>
            </div>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            However, with advances in computer vision, machine learning, and computer hardware, multimodal data collection has allowed for new, simplifying possibilities. For instance, computer vision algorithms can now accurately predict heart rate by computing
            minute skin tone differentials. More broadly, machine learning algorithms can now be used to develop easy-to-use and low-cost alternatives to conventional multimodal data collection.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üéØ The Goal</h3>
            <p>
              Our goal is to democratize multimodal data collection using advanced machine learning algorithms behind a user-friendly interface. We want to design a more intuitive, low-cost, and ethical solution compared to traditionally used physical sensor technologies.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üß© Our Solution</h3>
            <p>
              We developed the EZ-MMLA toolkit, which is a website for collecting multimodal data via video and audio input captured either in real-time or uploaded for post-processing. It is free to use and open to the public, so it can be used by all educational
              practitioners and researchers to collect multimodal student data. The types of data currently collected through the website are:
              <ol>
                <li>Body posture (skeletal data),</li>
                <li>Hand gestures,</li>
                <li>Attention (eye-tracking),</li>
                <li>Emotions,</li>
                <li>Physiological states (heart rate), and</li>
                <li>Lower-level computer vision algorithms (fiducial and color tracking).</li>
              </ol>

              The EZ-MMLA Toolkit uses machine learning models written in JavaScript to detect data via video and audio streams that run entirely within the web browser. A significant advantage of such models is that the computation required to train and run them is
              offloaded to the user‚Äôs device. This eliminates the need to maintain an expensive remote machine and allows our website to be scalable. It also allows for rapid real-time inferencing on the client-side, generating more comprehensive data
              and creating smoother user experiences. Furthermore, personal data does not leave the user‚Äôs devices; in our case, this means that webcam recordings remain within the browser and users do not have to worry about the security concerns of
              sending sensitive information over a network.

              <br><br>The EZ-MMLA Toolkit website has been designed with a user-friendly interface that does not require any pre-requisite technical knowledge. On the home page, users can select from an index of data-collection tools that correspond to
              a multimodal data type. When a user selects a tool, they are directed to a page where they can execute data collection in real-time through a webcam/microphone feed or upload a recording. Once the tracking is complete, the user is prompted
              to download the data as a CSV file.

              <br><br> <span style="font-weight: bold !important;">Comparing the EZ-MMLA toolkit with traditional data collection tools</span>
              <br>While computer-vision algorithms provide us with new ways of capturing multimodal data, they are not interchangeable with physical sensors. Generally, webcam-based data collection tools tend to be less accurate than dedicated sensors,
              but they are easier to access, distribute, setup, and use. They have also been steadily improving, as consumer hardware is becoming faster, higher resolution, and more affordable.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>‚úçÔ∏è User Feedback</h3>
            <p>
              There are several sources of user feedback that were collected during the semester. One source is anonymous weekly survey data from students: one question asked students to describe an aspect of class they enjoyed that week, the second was an aspect of
              class they felt needed improvement, and the third prompted students to report their experiences using the EZ-MMLA toolkit. Additionally, we included a usability instrument in the survey. Students were also prompted to give their feedback
              during class, sometimes orally and sometimes in a chat room.

              <br><br>A total of 504 open-ended response sets were collected over the course of 12 weeks, and 82 open-ended comments were identified as being relevant to the EZ-MMLA toolkit. We conducted thematic analysis on the data to identify key themes.
              Taking an inductive approach, preliminary codes were assigned to the data, which were then iteratively organized into two main themes (positive, negative) and several sub-themes on how students found the experience, shown briefly in the
              table below.
            </p>
          </div>

          <br><br>
          <div style="overflow-x:auto;">
            <table class="table-results">
              <caption>Some of the main themes identified from 504 response sets that were collected over the course of 12 weeks.</caption>
              <tbody>
                <tr>
                  <td style="font-family: 'Open Sans'; font-weight: bold; width: 20%;">Theme</td>
                  <td style="font-family: 'Open Sans'; font-weight: bold;">Examples</td>
                </tr>
                <tr>
                  <td>Authenticity</td>
                  <td>‚ÄúI liked that it was grounded in something real-world and relevant to the class‚Äù</td>
                </tr>
                <tr>
                  <td>Accessibility</td>
                  <td>‚ÄúThe fact that we got to actually use the Emotion Detecting tool and analyze it this week far exceeded my expectations in terms of what is possible to do as a student in our 3rd week of class!‚Äù</td>
                </tr>
                <tr>
                  <td>Technical Issues</td>
                  <td>‚ÄúThe eye tracking data collection website tend to make my laptop run slowly.‚Äù</td>
                </tr>
                <tr>
                  <td>Learning Curve</td>
                  <td>‚ÄúWhen I was collecting eye-gaze data from gazecloud, it took me several tries to figure out how I can both read and let gazecloud track my eyes.‚Äù
                  </td>
                </tr>
                <tr>
                  <td>Data Quality</td>
                  <td>‚ÄúI wish I was made more cognizant of how [the] quality [of] the data being collected is as I was being recorded (because I forgot a lot of the time). If there was a camera to show us how the camera is detecting our position, I might
                    change my behaviors so that I have my webcam screen on/not be leaning back out of view‚Äù
                  </td>
                </tr>
              </tbody>
            </table>
          </div>

          <div class="text-container">
            <p>
              <br><br><em>Usability.</em> To assess the usability of the website, we used Brooke‚Äôs ‚Äúquick and dirty‚Äù System Usability Scale (SUS) which comprises 10 questions that users rate on a 5 point scale from ‚ÄúStrongly Disagree‚Äù to ‚ÄúStrongly Agree‚Äù.
              Questions for example included ‚ÄúI found the system unnecessarily complex‚Äù or ‚ÄúI would imagine that most people would learn to use this system very quickly‚Äù. The final score is between 0 and 100, representing 7 levels from worst to best.
              29 students completed the SUS instrument. The final score of the EZ-MMLA website is 71.94 (SD = 14.84). The lowest scoring question was ‚ÄúI found the website unnecessarily complex‚Äù (mean = 3.6), which suggests that the website could be simplified
              from a user‚Äôs standpoint.

              <br><br><em>Authenticity.</em> Students seemed to appreciate that the data collected by the toolkit was, in the words of one student, ‚Äúmassive datasets that are generated by real MMLA tools.‚Äù Students were motivated that the data was authentic,
              and were also able to get their hands dirty in data analytics ‚Äî one student noted that ‚Äúthe opportunity to work with data collection directly‚Ä¶helps me to understand [the] limitations of data.‚Äù This perceived authenticity presents an opportunity
              for the EZ-MMLA toolkit in data science education; it is well known that authentic problems that are directly related to students foster engagement, motivation and deep understanding.

              <br><br><em>Accessibility.</em> Several students noted that the functionalities and data collected by the toolkit were intuitive to access and understand, despite their lack of prior expertise. One student stated that the activity using
              the emotion detection functionality of the toolkit ‚Äúfar exceeded my expectations in terms of what is possible to do as a student in our 3rd week of class‚Äù, and another felt the tool was ‚Äúsimple and straightforward to use‚Äù.

              <br><br><em>Technical issues.</em> The most prominent negative theme that emerged, on the other hand, was the frustration caused by technical issues. The most common issue was the website simply being slow and laggy for some students. A
              few reported that their laptops were running slower than usual when running the toolkit. The downloading function also caused some trouble, where webcam data could only be accessed by one application at a time.

              <br><br><em>Steep learning curve.</em> While some functionalities felt intuitive and accessible for some students, other functions felt difficult to grasp for others. Some students reported re-attempting data collection a few times before
              getting it right, and asked questions such as, ‚Äúhow can we check if data is being correctly collected?‚Äù Some students explicitly asked for examples and additional documentation. For instance, on collecting data for skeletal tracking, one
              comment was ‚ÄúI wish we knew an average number of frames that should be collected‚Äù.

              <br><br><em>Data quality.</em> A few comments pointed out the limitations of the data collected by the toolkit. One type of limitation arose from the unique context of students being privy to the exact setup and inner workings of the subject
              when working with their own data. This included concerns about how they had been conscious of the sensors and modeled their behavior to get ‚Äògood‚Äô data, or how they were concerned about the data being inaccurate because they were using a
              dual monitor setup, or had repeated the same experiment a few times before collecting the final dataset.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üí° Conclusion</h3>
            <p>
              In summary, this project finds that the results from the preliminary examination are encouraging. The platform has been broadly successful and indicative of how the proposed web-based approach might be viewed as a viable new medium for MMLA. Importantly,
              our findings suggest that there is the potential to overturn the conventional MMLA pipeline and democratize learning research.
            </p>
          </div>
        </div>
      </section>
    </div>
    <div class="scroll-to-top"></div>
  </main>
  <script type="text/javascript" src="../static/js/navbar.js"></script>
</body>

</html>