<!doctype html>
<html>

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>The EZ-MMLA Toolkit</title>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üë©üèª‚Äçüíª</text></svg>">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-1NG1BPBBQ1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-1NG1BPBBQ1');
  </script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script>
    $(function() {
      var includes = $('[data-include]')
      $.each(includes, function() {
        var file = $(this).data('include') + '.html'
        $(this).load(file)
      })
    })
  </script>

  <link rel="stylesheet" href="../style.css">
  <link rel="stylesheet" href="../style_extra.css">
  <link rel="stylesheet" href="../case-study.css">
  <link rel="stylesheet" href="../static/css/sticky_notes.css">

  <!-- fonts -->
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Rubik:wght@400;500;600;700;800;900&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;500;600;700;800;900&display=swap" rel="stylesheet">

  <!-- jquery -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

  <style>
     :root {
      --title-font: "Rubik", sans-serif;
      --body-font: 'Rubik', sans-serif;
    }
    /** new styles **/
    
    .ufo--case-study-container {
      --h1Font: "Rubik";
      --h1Color: #353738;
      --h1Weight: bold;
      --h1Style: normal;
      --h1Size: 24px;
      --h1Height: 25px;
      --h2Font: "Rubik";
      --h2Color: #353738;
      --h2Weight: normal;
      --h2Style: italic;
      --h2Size: 28px;
      --h2Height: 30px;
      --h3Font: "Rubik";
      --h3Color: #353738;
      --h3Weight: bold;
      --h3Style: normal;
      --h3Size: 21px;
      --h3Height: 25px;
      --h4Font: "Rubik";
      --h4Color: #353738;
      --h4Weight: normal;
      --h4Style: normal;
      --h4Size: 17px;
      --h4Height: 20px;
      --p1Font: "Open Sans";
      --p1Color: #353738;
      --p1Weight: normal;
      --p1Style: normal;
      --p1Size: 16px;
      --p1Height: 32px;
      --p2Font: "Rubik";
      --p2Color: #7b7e80;
      --p2Weight: normal;
      --p2Style: normal;
      --p2Size: 16px;
      --p2Height: 25px;
      --projectTitleFont: "Rubik";
      --projectTitleColor: #252829;
      --projectTitleWeight: bold;
      --projectTitleStyle: normal;
      --projectTitleSize: 35px;
      --projectTitleHeight: 45px;
      --projectSubTitleFont: "Open Sans";
      --projectSubTitleColor: #252829;
      --projectSubTitleWeight: normal;
      --projectSubTitleStyle: normal;
      --projectSubTitleSize: 16px;
      --projectSubTitleHeight: 28px;
    }
    
    @media (min-width: 576px) {
      .ufo--case-study-container {
        --h3Size: 25px;
        --h3Height: 28px;
        /* --p1Size: 18px;
        --p1Height: 21px; */
        --projectTitleSize: 48px;
        --projectTitleHeight: 51px;
        --projectSubTitleSize: 22px;
        --projectSubTitleHeight: 37px;
      }
    }
    
    @media (min-width: 768px) {
      .ufo--case-study-container {
        --h1Size: 60px;
        --h1Height: 63px;
        --h2Size: 40px;
        --h2Height: 43px;
        --h3Size: 30px;
        --h3Height: 36px;
        --h4Size: 24px;
        --h4Height: 29px;
        /* --p1Size: 18px;
        --p1Height: 30px; */
        --p2Size: 16px;
        --projectTitleSize: 88px;
        --projectTitleHeight: 91px;
        --projectSubTitleSize: 22px;
        --projectSubTitleHeight: 37px;
      }
    }
    
    .p_section--header-main .ufo--hero-content-grid.has-image .text-container {
      text-align: center;
    }
    
    .p_section--header-main .text-container {
      text-align: center;
    }
    
    @media (min-width: 768px) {}
    
    @media (min-width: 992px) {
      .p_section--header-main .ufo--hero-content-grid.has-image .text-container {
        text-align: left;
      }
    }
    
    td {
      display: table-cell;
      align-items: center;
      /* border: 1px dotted red;
      background-color: red; */
    }
    
    .hero-main-content {
      max-width: 1240px;
      padding: 0px 20px 0px 20px;
      margin-left: auto;
      margin-right: auto;
    }
    
    .hero-text {
      width: 60%;
      padding-right: 20px;
    }
    
    .hero-image {
      text-align: center;
      vertical-align: bottom;
    }
    
    .hero-image img {
      height: 550px;
      width: auto;
    }
    
    @media only screen and (max-width: 768px) {
      .hero-main-content td {
        display: block;
      }
      .hero-text {
        width: 100%;
        text-align: center;
        margin-top: 10px;
      }
      .hero-image img {
        width: 50%;
        height: auto;
        margin-top: 20px;
      }
      .hero-text h1 {
        margin-bottom: 10px;
      }
    }
  </style>
</head>

<body>
  <main>
    <div class="ufo--case-study-container project">

      <section id="section-0" class="ufo--case-study-section p_section p_section--header-main  " style="background-color: #aecdbf; margin-bottom: 40px;">
        <div class="ufo--container">
          <div class="my-ufo--page-navbar-container">
            <div class="ufo--page-navbar">
              <div class="ufo--page-navbar-container" data-include="navbar"></div>
              <div class="ufo--page-navbar-toggle"><span></span><span></span><span></span><span></span></div>
            </div>
          </div>

          <table class="hero-main-content" width="100%" height="100%" style="padding-top: 5%;" cellpadding="0" cellspacing="0">
            <tbody>
              <tr>
                <td class="hero-text">
                  <h1>The EZ-MMLA Toolkit</h1>
                  <h2>A data collection website that provides educators and researchers with easy access to multimodal data streams.</h2>
                </td>
                <td class="hero-image">
                  <img src="../img/man-cropped.png">
                </td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>‚úèÔ∏è Overview</h3>
            <p>
              I am working with a group of engineers and researchers at the <a target="_blank" href="https://lit.gse.harvard.edu/">Learning, Innovation, and Technology lab</a> based at the Harvard University to develop a <a target="_blank" href="https://mmla.gse.harvard.edu/">multimodal data collection
              website</a>. The website uses state-of-the-art machine learning algorithms to collect data on students‚Äô body posture, hand gestures, attention, emotions, and physiological states. The application is currently being used by 100+ Harvard graduate
              students enrolled in a Multimodal Analytics course.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üë©üèª‚Äçüíª My Role</h3>
            <p>
              I am the team lead for this project. I defined the product roadmap, researched the most effective models to use in the application, developed the frontend for the majority of tools currently available on the website, architected and developed the backend,
              and deployed the application. I also provided technical guidance and mentorship to team members.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üîé Background</h3>
            <p>
              Data has proven valuable in understanding student learning processes and uncovering factors that most affect learning outcomes. <span class="highlight">Recently, educational researchers have become more interested in new tools for capturing detailed learning trajectories, which
              has led to the creation of a new methodology: Multimodal Learning Analytics (MMLA)</span>, involving the computational analysis of multimodal data. Generally, MMLA research focuses on using high-frequency sensors and wearable tracking devices,
              such as eye-trackers (e.g., Tobii eye-trackers) and motion sensors (e.g., Microsoft Kinect), to capture rich physiological and behavioral student data.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üö© The Problem</h3>
            <p>
              While MMLA is a promising field, it is not a widely used methodology in practice because of the specialized equipment and technical knowledge required to collect, process, and analyze multimodal datasets. Multimodal data collection, in particular, involves
              the use of a combination of physical sensors, proprietary tools, and external contractors, which pose the following challenges:
            </p>
          </div>
        </div>
      </section>

      <section id="section-2" class="ufo--case-study-section p_section" style="padding-top: 20px;  padding-bottom: 20px;">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <div class="ufo--basic-text-area">
              <ul class="sticky_notes">
                <li>
                  <a>
                    <p><b>üí∞ Affordability</b></p>
                    <p>Current sensor hardware is generally not affordable by researchers and teachers. This restricts multimodal data research to well-funded teams or more commercially lucrative research areas.</p>
                  </a>
                </li>
                <li>
                  <a>
                    <p><b>üîí Accessibility</b></p>
                    <p>The high-effort and logistically challenging processes of data collection and analysis restrict access to users with a strong technical background.</p>
                  </a>
                </li>
                <li>
                  <a>
                    <p><b>üë§ Participant Effects</b></p>
                    <p>Physical sensors can be invasive for participants and lead to Hawthorne effects and similar biases entering data.</p>
                  </a>
                </li>
                <li>
                  <a>
                    <p><b>‚ùó Limited research potential</b></p>
                    <p>The complexity and cost of the data collection process make it difficult to develop a system of frequent assessment and feedback, which is an essential process in education.</p>
                  </a>
                </li>
              </ul>
            </div>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üé® Product Development Process</h3>
            <p>
              I adopted the following product development process. Since I had limited time before the product launch, I focused most of my time on development and iterating over that.
            </p>
          </div>
          <br><br>
          <div class="img_container">
            <img style="height: 200px;" src="/img/ez-mmla/flow.svg">
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>Ideation</h3>
            <p>
              Our target is to democratize multimodal data collection. Based on this target and our problem statement, we identified the key business goals for our product. That is, we want to design a solution that is:
            </p>
            <ul>
              <li>intuitive</li>
              <li>low-cost, and </li>
              <li>ethical</li>
            </ul>
            <p>
              compared to traditionally used physical sensor technologies.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>Research</h3>
            <p>
              As a starting point, we each did some research on existing products in this area to investigate the current offerings in the market and take inspiration from particular features that we liked about each.
            </p>
            <p>
              Particularly, we focused our research efforts on current advancements in computer vision, machine learning, and computer hardware to allow for new, simplifying possibilities. For instance, computer vision algorithms can now accurately predict heart rate
              by computing minute skin tone differentials. More broadly, <span class="highlight">machine learning algorithms can now be used to develop easy-to-use and low-cost alternatives to conventional multimodal data collection.</span>
            </p>
            <p>Part of our research involved examinating existing computer-vision algorithms against physical sensors. The table below provides a summary of the differences between these two approaches for a few modalities.
            </p>
          </div>

          <br><br>
          <div style="overflow-x:auto;">
            <table class="table-results">
              <caption>Comparison of Physical and Webcam-based sensors across different modalities. Exemplar tools are underlined. Pros and cons are described below each tool.</caption>
              <tbody>
                <tr>
                  <td style="font-family: 'Open Sans'; font-weight: bold; width: 20%;">Modality</td>
                  <td style="font-family: 'Open Sans'; font-weight: bold;">Physical Sensor</td>
                  <td style="font-family: 'Open Sans'; font-weight: bold;">Webcam-Based Sensor</td>
                </tr>
                <tr>
                  <td>Skeletal Tracking</td>
                  <td>
                    <u>Kinect v2 ¬©</u>
                    <br>Pro: 3D tracking
                    <br>Con: Noisy in case of occlusions; discontinued
                  </td>
                  <td>
                    <u>PoseNet</u>
                    <br> Con: 2D tracking (no depth)
                    <br> Pro: more accurate in case of occlusions
                  </td>
                </tr>
                <tr>
                  <td>Eye-Tracking</td>
                  <td>
                    <u>Tobii 4C/5 ¬©</u>
                    <br> Pro: accurate
                    <br> Con: cost
                  </td>
                  <td>
                    <u>Webgazer</u>
                    <br> Pro: can be integrated to any website
                    <br> Con: less accurate
                  </td>
                </tr>
                <tr>
                  <td>Hand tracking</td>
                  <td>
                    <u>Leap Motion ¬©</u>
                    <br> Pro: easily integrated to any headset
                  </td>
                  <td>
                    <u>Google AI Hand Tracking</u>
                    <br> Pro: as accurate as physical sensors
                    <br> Con: slow for multiple hands
                  </td>
                </tr>
              </tbody>
            </table>
          </div>

          <br>
          <div class="text-container">
            <p>
              Sometimes webcam-based tools are equivalent to (or better than) dedicated sensors, and sometimes dedicated sensors provide better data. On the whole, large machine learning applications are able to operate moderately well online in real-time with support
              from open-source development teams such as Google‚Äôs TensorFlow.js. As such, <span class="highlight">our proposed approach for multimodal data collection is to utilize web-based tools and machine learning models for data collection.</span>
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>Planning</h3>
            <p>
              I created proof-of-concepts to test our idea and demonstrate feasibility, which are described below.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h4>Persona Development</h4>
            <p>
              Our platform is intended to be used by teachers, students and researchers for capturing multimodal data during web-based experiments and in the classroom. For instance, the tool has been used with consent to collect data from students in a data analysis
              course to gain insights for teaching and to also serve as raw data for data analysis assignments.
            </p>
            <p>
              I developed primary personas demonstrating our target users, especially their goals and pain points. I referred to them throughout the entire product development process.
            </p>
          </div>

          <br><br>
          <div class="img_container">
            <img style="max-height: 400px;" src="/img/ez-mmla/Persona.png">
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h4>Wireframing</h4>
            <p>
              I quickly mocked up some basic wireframes to gather feedback from the team and the users on the overall layout and structure of our app.
            </p>
          </div>
          <div class="img_container">
            <img style="max-height: 400px;border: 1px solid #e8e8e8;" src="/img/ez-mmla/wireframe 1.png">
          </div>
          <div class="img_container">
            <img style="max-height: 400px;border: 1px solid #e8e8e8;" src="/img/ez-mmla/wireframe 2.png">
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h4>Validating the Designs</h4>
            <p>
              I conducted usability testing sessions with our primary users to validate whether the new designs would solve their problems. During the sessions, I observed how they interacted with the prototypes and made adjustments accordingly.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üß© Development</h3>
            <p>
              Finally, we developed the EZ-MMLA toolkit, which is a website for collecting multimodal data via video and audio input captured either in real-time or uploaded for post-processing. It is free to use and open to the public, so it can be used by all educational
              practitioners and researchers to collect multimodal student data. The types of data currently collected through the website are:
            </p>
            <ol>
              <li>Body posture (skeletal data),</li>
              <li>Hand gestures,</li>
              <li>Attention (eye-tracking),</li>
              <li>Emotions,</li>
              <li>Physiological states (heart rate), and</li>
              <li>Lower-level computer vision algorithms (fiducial and color tracking).</li>
            </ol>
            <p>
              <span class="highlight">The EZ-MMLA Toolkit uses machine learning models written in JavaScript to detect data via video and audio streams that run entirely within the web browser.</span> A significant advantage of such models is that the
              computation required to train and run them is offloaded to the user‚Äôs device. This eliminates the need to maintain an expensive remote machine and allows our website to be scalable. It also allows for rapid real-time inferencing on the client-side,
              generating more comprehensive data and creating smoother user experiences. Furthermore, personal data does not leave the user‚Äôs devices; in our case, this means that webcam recordings remain within the browser and users do not have to worry
              about the security concerns of sending sensitive information over a network.
            </p>
            <p>
              The EZ-MMLA Toolkit website <span class="highlight">has been designed with a user-friendly interface that does not require any pre-requisite technical knowledge.</span> On the main page, users can select from an index of data-collection
              tools that correspond to a multimodal data type. When a user selects a tool, they are directed to a page where they can execute data collection in real-time through a webcam/microphone feed or upload a recording. Once the tracking is complete,
              the user is prompted to download the data as a CSV file.
            </p>
          </div>
          <div class="img_container">
            <div style="width: 785.42px; overflow:hidden; display:block; height: 390px; border: 1px solid #e8e8e8;">
              <video height="400" muted autoplay loop>
                <source src="/img/ez-mmla/main-page.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <p class="caption">Home page.</p>
          </div>
          <div class="img_container">
            <div style="width: 785.42px; overflow:hidden; display:block; height: 390px; border: 1px solid #e8e8e8;">
              <video height="400" muted autoplay loop>
                <source src="/img/ez-mmla/tools-page.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <p class="caption">Tools page. This page contains an index of data collection tools the user can select from.</p>
          </div>
          <div class="img_container">
            <div style="width: 785.42px;">
              <img style="width: 785.42px; border: 1px solid #e8e8e8;" src="/img/ez-mmla/posenet.png">
              <p class="caption">Main data collection page. This page allows the user to collect data from the PoseNet model via a webcam feed or video recording.</p>
            </div>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>‚úçÔ∏è Testing & User Feedback</h3>
            <p>
              In the Fall 2020, <span class="highlight">the EZ-MMLA toolkit was used in a course on Multimodal Learning Analytics at Harvard University.</span> There are several sources of user feedback that were collected during the semester. One source
              is anonymous weekly survey data from students: one question asked students to describe an aspect of class they enjoyed that week, the second was an aspect of class they felt needed improvement, and the third prompted students to report their
              experiences using the EZ-MMLA toolkit. Additionally, we included a usability instrument in the survey. Students were also prompted to give their feedback during class, sometimes orally and sometimes in a chat room.
            </p>
            <p>
              <span class="highlight">A total of 504 open-ended response sets were collected over the course of 12 weeks</span>, and 82 open-ended comments were identified as being relevant to the EZ-MMLA toolkit. We conducted thematic analysis on the
              data to identify key themes. We organized the responses into two main themes (positive, negative) and several sub-themes on how students found the experience, shown briefly in the table below.
            </p>
          </div>

          <br><br>
          <div style="overflow-x:auto;">
            <table class="table-results">
              <caption>Some of the main themes identified from 504 response sets that were collected over the course of 12 weeks.</caption>
              <tbody>
                <tr>
                  <td style="font-family: 'Open Sans'; font-weight: bold; width: 20%;">Theme</td>
                  <td style="font-family: 'Open Sans'; font-weight: bold;">Examples</td>
                </tr>
                <tr>
                  <td>Authenticity</td>
                  <td>‚ÄúI liked that it was grounded in something real-world and relevant to the class‚Äù</td>
                </tr>
                <tr>
                  <td>Accessibility</td>
                  <td>‚ÄúThe fact that we got to actually use the Emotion Detecting tool and analyze it this week far exceeded my expectations in terms of what is possible to do as a student in our 3rd week of class!‚Äù</td>
                </tr>
                <tr>
                  <td>Technical Issues</td>
                  <td>‚ÄúThe eye tracking data collection website tend to make my laptop run slowly.‚Äù</td>
                </tr>
                <tr>
                  <td>Learning Curve</td>
                  <td>‚ÄúWhen I was collecting eye-gaze data from gazecloud, it took me several tries to figure out how I can both read and let gazecloud track my eyes.‚Äù
                  </td>
                </tr>
                <tr>
                  <td>Data Quality</td>
                  <td>‚ÄúI wish I was made more cognizant of how [the] quality [of] the data being collected is as I was being recorded (because I forgot a lot of the time). If there was a camera to show us how the camera is detecting our position, I might
                    change my behaviors so that I have my webcam screen on/not be leaning back out of view‚Äù
                  </td>
                </tr>
              </tbody>
            </table>
          </div>

          <div class="text-container">
            <br>
            <p>
              <b>Usability.</b> To assess the usability of the website, we used Brooke‚Äôs ‚Äúquick and dirty‚Äù System Usability Scale (SUS) which comprises 10 questions that users rate on a 5 point scale from ‚ÄúStrongly Disagree‚Äù to ‚ÄúStrongly Agree‚Äù. Questions
              for example included ‚ÄúI found the system unnecessarily complex‚Äù or ‚ÄúI would imagine that most people would learn to use this system very quickly‚Äù. The final score is between 0 and 100, representing 7 levels from worst to best. 29 students
              completed the SUS instrument. The final score of the EZ-MMLA website is 71.94 (SD = 14.84). The lowest scoring question was ‚ÄúI found the website unnecessarily complex‚Äù (mean = 3.6), which suggests that the website could be simplified from
              a user‚Äôs standpoint.
            </p>

            <p>
              One pain point we discovered during our usability tests was that users found the sidebar on model configurations to be intimidating when first opening the main data collection page. Therefore, we hid those settings in a sidebar to the left of the page,
              which, when needed, could be opened up by users as they get more comfortable with the tool. This is illustrated below.
            </p>
          </div>

          <div class="img_container">
            <div style="overflow-y:hidden; display:block; height: 400px; border: 1px solid #e8e8e8;">
              <video height="400" muted autoplay loop>
                    <source src="/img/ez-mmla/settings.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
            </div>
            <p class="caption">The model configuration settings are hidden when the user first lands on the page, but can be displayed when needed.</p>
          </div>

          <div class="text-container">
            <p>
              <b>Authenticity.</b> Students seemed to appreciate that the data collected by the toolkit was, in the words of one student, ‚Äúmassive datasets that are generated by real MMLA tools.‚Äù Students were motivated that the data was authentic, and
              were also able to get their hands dirty in data analytics ‚Äî one student noted that ‚Äúthe opportunity to work with data collection directly‚Ä¶helps me to understand [the] limitations of data.‚Äù This perceived authenticity presents an opportunity
              for the EZ-MMLA toolkit in data science education; it is well known that authentic problems that are directly related to students foster engagement, motivation and deep understanding.
            </p>
            <p>
              <b>Accessibility.</b> Several students noted that the functionalities and data collected by the toolkit were intuitive to access and understand, despite their lack of prior expertise. One student stated that the activity using the emotion
              detection functionality of the toolkit ‚Äúfar exceeded my expectations in terms of what is possible to do as a student in our 3rd week of class‚Äù, and another felt the tool was ‚Äúsimple and straightforward to use‚Äù.
            </p>
            <p>
              <b>Technical issues.</b> The most prominent negative theme that emerged, on the other hand, was the frustration caused by technical issues. The most common issue was the website simply being slow and laggy for some students. A few reported
              that their laptops were running slower than usual when running the toolkit. The downloading function also caused some trouble, where webcam data could only be accessed by one application at a time.
            </p>
            <p>
              <b>Steep learning curve.</b> While some functionalities felt intuitive and accessible for some students, other functions felt difficult to grasp for others. Some students reported re-attempting data collection a few times before getting
              it right, and asked questions such as, ‚Äúhow can we check if data is being correctly collected?‚Äù Some students explicitly asked for examples and additional documentation. For instance, on collecting data for skeletal tracking, one comment
              was ‚ÄúI wish we knew an average number of frames that should be collected‚Äù.
            </p>
            <p>
              <b>Data quality.</b> A few comments pointed out the limitations of the data collected by the toolkit. One type of limitation arose from the unique context of students being privy to the exact setup and inner workings of the subject when
              working with their own data. This included concerns about how they had been conscious of the sensors and modeled their behavior to get ‚Äògood‚Äô data, or how they were concerned about the data being inaccurate because they were using a dual
              monitor setup, or had repeated the same experiment a few times before collecting the final dataset.
            </p>
          </div>
        </div>
      </section>

      <section class="ufo--case-study-section p_section p_section--text" style=" ">
        <div class="ufo--p-container-fluid">
          <div class="text-container">
            <h3>üí° Results & Takeaways</h3>
            <p>
              The platform has been broadly successful and indicative of how the proposed web-based approach might be viewed as a viable new medium for MMLA. It has become a permanent part of a Data Analytics course offered at Harvard University to graduate students.
              <span class="highlight">The application helped to cut down classroom costs from $2000+ in physical sensors to $0 in webcam-based sensing </span> in the browser using JavaScript Machine Learning models. While there is still room for improvement,
              our results and findings suggest that there is the potential to overturn the conventional MMLA pipeline and democratize learning research.
            </p>
            <p>
              Some key takeaways from this project are:
            </p>
            <ol>
              <li>
                <b>Create a strategic plan to launch a product.</b> This helps to deliver a quality product when there is limited time.
              </li>
              <li>
                <b>User testing doesn't end after development.</b> Design is a constant iteration of improving the experience for the end user. Always find ways to collect and listen to your user's feedback.
              </li>
            </ol>
          </div>
        </div>
      </section>
    </div>
    <div class="scroll-to-top"></div>
  </main>
  <script type="text/javascript" src="../static/js/navbar.js"></script>
</body>

</html>